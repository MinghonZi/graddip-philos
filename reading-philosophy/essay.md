<div align="center">
  <h1>Is the question of trust applicable to artificial agents?</h1>
  commented by Miriam Bowen
</div>

## Feedback from Miriam Bowen (Mark: 15)

In this essay you argue that trust is something that exists between human and artificial agents. You do this by considering some objections to the view that there is trust between human and artificial agents: the argument that it is mere reliance and the argument that artificial agents lack motivation. You then give positive accounts of trust in terms of trust pluralism and a focus the social-phenomenological approach to trust.

The writing was generally clear and there was good engagement with some of the trust literature and good use of examples to illustrate your points. You show some good critical engagement – it would have been good to see you develop this further. For example, considering how someone might respond to the arguments that you’re giving.

A weakness of the essay was a lack of definition of artificial agent. This made it unclear what the scope of the aims of the essay were. If artificial agents are genuine artificial intelligence then it seems the problem collapses back into the same question of how we can define human-human trust. If it’s not AI then I need some idea of the scope of what and artificial agent can do in order to understand the force of some of your arguments. A little more structure on the different accounts of trust you considered/ would be considering would have also helped the reader follow along with some of the later arguments.
A good effort and lots of interesting ideas and arguments packed in – just could have done with slowing down on some of them at times and really explaining and evaluating them further.

## Introduction

Humans, in pursuit of meeting certain needs that manually operated artefacts could not fulfil, began to create entities with agency, known as artificial agents. With traditional agents, like humans and dogs, trust issues exist, such as the possibility that although humans can control dogs, the dogs might disobey and cause harm. Philosophers have questioned whether trust relationships can exist with artificial agents, as these entities lack certain qualities found in humans and animals, such as motivation, morality, and emotions, which are potentially essential elements of trust. In other words, the form of dependence between humans and artificial agents might only be mere reliance, not trust. In this essay, I will defend that the question of trust is still applicable to artificial agents.
> Good - well done for outlining what you'll be arguing for in the essay.

This article is organized as follows. In Section 2, I will address the threat posed by <mark>verification</mark>, arguing that it cannot completely downgrade <mark>Human-AA</mark> trust to mere reliance. In Section 3, I will attempt to resolve the issue of trust establishment requirement being unmet due to the lack of motivation in artificial agents. Even if my response in Section 3 is unsuccessful, in Section 4, I propose trust pluralism as a way to circumvent the problems faced in Section 3. Section 5 discusses a different perspective from the establishment of trust, known as the social-phenomenological approach to trust, because under this view, the question of Human-AA trust is certainly applicable. In Section 6, I shift the focus from whether trust really exists between humans and artificial agents to how artificial agents can manipulate human trust, such as causing people to establish trust that should not exist, thus reinforcing the necessity to answer the question of trust involving artificial agents. Section 7 concludes this essay.
> What is verification - is it a technical concept you're using here?

> If you're going to use an abbreviation in the essay you should introduce it somewhere i.e artificial agents (AA).

> Good an clear structure and outline of what you'll be doing in the essay.

## Verification

<mark>Most philosophers agree that trust involves vulnerability</mark> (McLeod, 2020). A common example is that we may trust an application with access to our phone's camera, microphone, photo gallery, and GPS to enable certain functionalities. This trust exposes our privacy which has potential risks. Some technologists believe that we can verify artefacts to eliminate the need for trust, thereby removing the risk of exposing vulnerability (Sullins, 2020). e slogan for this propositional attitude is "zero trust, always verify". Unlike the common attitude of assuming something works until proven otherwise and then adjusting one's trust accordingly, verification operates under the assumption that nothing functions properly and requires verification of functionality before operation. I shall argue that verification cannot completely downgrades Human-AA trust to mere reliance.
> Is this in relation to human-human interpersonal trust relationships?
>
> It might have been useful to start with an example of human-human trust to establish what sort of phenomena or concept you're trying to latch onto.

Every artefact is manufactured according to certain specifications. One form of verification is an answer to the question that does X conform to the specifications (Huang, 2020). Through verification, trust in artefacts can be transformed into reliance on their specifications and trust in the authority conducting the verification. As long as artefacts are used only within the scope explicitly defined in their specifications, the trust relationship involved in this situation remains interpersonal. However, due to the <mark>dynamic nature of artificial agents and the environments in which they are deployed</mark>, we often cannot guarantee that they will not encounter situations not specified in its specification, where behaviour becomes unpredictable. For example, Uber's test self-driving car fatally struck a jay walker because the engineers had not considered jaywalking possibilities in the design. It appears that transforming trust into reliance on specifications through verification is only suitable for artefacts whose specifications encompass all intended uses, but not applicable to highly dynamic artificial agents. High dynamism, in other words, means that an artificial agent has a vast state space, making it impractical to explore all states and transitions and verify them.
> I'm not clear on what concept of an artificial agent you're working with.

> Why is this not a case of the car not being reliable?
>
> Or couldn't someone argue that it's still reliable at driving just not reliable at not hitting people? 
>
> This could have been explained further.

Another form of verification, which can address this high dynamism, is checking conformance of the specifications defined by the human for the current situation before or during the operation of an artificial agent. Imagine the following scenario: A driver encounters a challenging parking space, but the car is equipped with a remote automated parking feature. The driver gets out of the car, activates the automated parking function using the smartphone, and observes the car's automatic manoeuvres around the parking space for any potential collisions. If there is a risk of a collision, the driver can use the smartphone to immediately stop the automated parking process. In this example, there is no verification prior to activating the parking function to guarantee that the car can successfully park in the designated spot. The verification process occurs through the driver's continuous observation of the car's automated movements. With each manoeuvre of the car, the driver mentally anticipates potential collision points and then moves to the corresponding area to verify. This form of verification seems downgrade trust in the artificial agent to continuous supervision of its actions.

I have argued that the first form of verification is impractical for completely downgrading trust in an artificial agent to reliance on specifications due to the vast state space that makes it difficult to for designers to provide a comprehensive specification. The second form of verification, although it breaks down the entire state space so that each manoeuvre holds a specification that can be verified individually, does not eliminate the trust a human has in an artificial agent. In the example, when the driver activates the automated parking feature, this action already signifies his trust that the autonomous car might be able to park itself in the challenging spot. The driver's continuous verification does not remove trust, but rather it addresses the uncertainty in his trust. Therefore, Human-AA trust cannot be eliminated by verification for the sake of reducing vulnerability.
> I'm not clear on why in the 2nd case it's also not reliance.

## Motivation

From the previous discussion, we can discern that trust requires satisfying two conditions: 1) being vulnerable 2) relying on a competence in what we wish to accomplish. Verification achieves the effect of eliminating Human-AA trust by confirming artificial agents’ competence and removing human’s vulnerability. Trust also involves a third condition: 3) relying on the motivation of trustees to do what we wish them to do (McLeod, 2023). If someone is competent to do what we wish, but we know they are not motivated, then we cannot trust that they will actually do it. However, an artificial agent may not possess motivation. If an artificial agent lacks motivation, then according to the third requirement of trust, the Human-AA trust relationship should never be established.
> It would have been good to acknowledge that you're committing yourself to a particular definition of trust - there's disagreement in the trust literature.

Trust is often seen as involving a motivation such as goodwill, benevolence, or moral integrity of the other party. This means that when we trust someone, we do so because we believe their motivations align with positive moral values for us. For instance, we trust a friend to keep a secret because we believe they value our friendship and our privacy, not just because they are capable of keeping secrets. Reliance, on the other hand, can exist without these moral considerations. For example, we might rely on a bus to arrive on time based on its schedule, but this does not involve any belief in the bus driver's personal goodwill towards us. Here, our belief is based on evidence of regularity, not a moral judgment of the bus driver or the transport system.
> One might argue that in this example there is a view about the bus drivers goodwill towards something...

Coeckelbergh (2012) argues that artificial agents lack human qualities and character traits that are prerequisite for being capable and worthy of trust. We cannot establish the same kind of trust relationship with artificial agents that humans can establish with other humans. Nevertheless, Coeckelbergh acknowledges that humans may establish a so-called 'quasi-trust' relationship with artificial agents. This notion of 'quasi-trust' indicates a form of dependence that does not fulfil all the traditional criteria of trust. In this view, we may depend on artificial agents for certain tasks without attributing to them motivations or moral qualities.
> Good - well explained. Is the quasi-trust just the 2 criteria without the motivation?

The lack of inherent moral agency in artificial agents indeed complicates the question of whether trust can be established in Human-AA interactions. The introduction of the concept of quasi-trust further adds to this complexity. Philosophers argue that the difference between trust and mere reliance involves some additional factors, making quasi-trust a concept that lies between trust and mere reliance. Quasi-trust includes more factors than mere reliance but fewer than full trust. This suggests a gradual spectrum between reliance and trust, with any point in between representing a form of quasi-trust. However, intuitively, when we trust, we do not perceive our trust as a quasi-trust that falls between reliance and trust. Similarly, when we feel we trust artificial agents, we do not perceive this trust as a downgraded version of the trust we typically have in humans. My stance is that there is no Human-AA quasi-trust.

Rejecting the notion of quasi-trust and considering that Human-AA trust does not fully meet the criteria of trust, it might seem that only Human-AA reliance exists. However, if one insists on the existence of Human-AA trust and accepts the third requirement of trust, then we must consider where the motivation in Human-AA trust originates. There are two perspectives to introduce motivation into the human-AA relationship. Firstly, when we trust an artificial agent, the object of trust includes not only the agent but also the humans behind it, and the motivation is essentially those human’s motivation. Here, we consider the artificial agent and its associated humans as one entity. The most obvious example is that the trust relationship integrates the motivation of agent’s designer and owner. A less obvious example is when hackers penetrate artificial agents, shifting the hidden motivation to those of the hackers. There are hidden layers of the trust towards artificial agents. On the surface, artificial agents may indeed seem motiveless, but under the surface, their motivations extend to the humans they are connected with. The second perspective considers that humans can act as if the artificial agents did have certain motivation and interact with them in false. In educational settings, children might be encouraged to treat the artificial agent designed to instruct them as a peer, thanking it for help or asking it questions as if it were a knowledgeable friend. Although we can criticise a child's trust in the artificial tutor is misplaced, it is an economical, because discovering the motivation of the humans behind the agent is often not easy.
> Good - these are clear examples. How do you think someone might respond to them? 
>
> Is there an issue about the level of responsibility the agents behind them are attributed?

In summary, I reject the notion of quasi-trust and suggest that Human-AA trust might exist if the motivation component is considered as originating from the humans behind the artificial agent. For instance, the trust in an artificial agent could be an extension of trust in its designers, owners, or even potential hackers. <mark>Alternatively, humans might interact with artificial agent as if they had motivations.</mark> These two perspectives can prevent Human-AA trust from failing to be established under the traditional requirements of trust.
> How is this motivation? The issue still remains on this second reading.

## Trust Pluralism

While I refuted the concept of quasi-trust in the previous section, I did not dismiss the intuition that trust in artificial agents is distinct from both human trust and mere reliance. This intuitive distinction suggests the possibility that trust in artificial agents might be a unique form of trust. In fact, there is even no consensus on a singular definition of trust in human-to-human relationships (Sullins, 2020), leading to the emergence of trust pluralism, which posits that there may be more than one kind of trust. <mark>If we move away from the idea of a singular form of trust, we can explore whether Human-AA trust constitutes a unique form of trust.</mark> The first to delve into this question was Taddeo (2010).
> So one thing to ask here is why think this is the same phenomena as human-human trust or interestingly connected in any way?

Taddeo introduced a form of trust, termed e-Trust, which is based on the premise that all agents are rational, and trust is a product of rational decision-making process. Taddeo emphasises that e-Trust must not occur a priori; it is established only after assessing the agent's trustworthiness. This trustworthiness is supported by evidence. Trustworthiness is not a general value but is goal specific, which means its value varies according to the different objectives one expects the agent to achieve. The evidence supporting trustworthiness may include the set of actions an agent is likely to take towards a particular goal. Taddeo's e-Trust tightly binds the concepts of trust and trustworthiness and entrusting the assessment of trustworthiness to a scientific process underpinned by evidence. The e-Trust theory appears akin to game theory. While Taddeo's theory provides a clear framework, it has limitations because it does not account for emotional and moral factors. <mark>As a result, some trust phenomena cannot be explained within this framework.</mark>
> Can you give an example?

Grodzinsky et al. (2020) recognised the dilemma in researching trust: no matter how trust is defined, there are always phenomena that this definition cannot explain. They proposed an object-oriented model of trust to resolve this dilemma. This model assumes the plurality of Trust and establishes a tree-like hierarchical structure for it. At the root of this structure is the most abstract form of trust, termed Trust*, which possesses attributes common to all other forms of trust. Other forms of trust are more specific expressions of Trust*. For example, common to both Human-AA Trust and Human-Human Trust is the judgment of competence, but they differ in the judgment of morality. If there were only these two forms of Trust in the world, then judgment of competence would be attributed to Trust*, while judgment of morality would be attributed to Human-Human Trust, and not to Human-AA Trust.

This object-oriented model of trust is more of a research framework than a model of trust. We clarify our understanding of trust by continuously identifying the possible forms of trust and their shared and distinct attributes. It is important to note that <mark>Trust* in this model is not the definitive concept of trust we seek; it is merely the common part of all known forms of trust.</mark> Since we do not know how many forms of trust exist in the world, the definition of Trust* is always incomplete. In other words, following this framework, we are continually collecting various potential forms of trust, such as Human-Human Trust, Human-AA Trust, Human-Alien Trust, Human-Computer Trust, without ever arriving at a singular answer to what Trust is. Although this approach acknowledges the existence of Human-AA Trust, it offers limited philosophical value.
> Couldn't this end up being a very thin notion that's uninteresting?

## Social-Phenomenological Approach to Trust

In previous sections, the discussion about Human-AA trust primarily revolved around whether such a unique form of trust could be established. Coeckelbergh (2012) referred to this perspective on trust as the contractarian-individualist approach, characterized by viewing trust as a product of an individual's decision. However, when we trust, it is often not a conscious choice. Based on this, Coeckelbergh proposed that trust might not necessarily be established by individuals but could instead exist inherently within the social environment. In other words, in this view, trust is objectively embedded in social relationships. When humans establish a certain social relationship with an artificial agent, pre-existing trust manifests itself. For example, the trust developed between humans and dogs through a long history of interaction; if a robotic dog is created that makes humans feel like it is, or resembles, a real dog, and <mark>they develop a social relationship similar to that with a real dog,</mark> then the trust manifested between the human and the robotic dog would be akin to that between a human and a real dog. <mark>Similarly, if a humanoid robot is created that leads to the development of human-human-like social relationship, then the trust between the human and the humanoid robot would mirror the trust embedded in human-human social relationship.</mark>
> Is it supposed to be a two sided relationship? Or is it only focused on whether the human perceives it as a social relationship even if there's no relationship in the artificial agent's view.

> This seems like it's just going to run into the same motivation problem you gave earlier - if someone wasn't convinced by those arguments then is there just the same problem in a different form here?

Coeckelbergh termed this perspective social-phenomenological approach to trust. It offers a viable way to explain the existence of Human-AA trust beyond mere reliance. When an artificial agent establishes a human-dog-like social relationship with humans, the trust relationship manifests as the trust humans have in dogs. We could also create an agent unlike anything in the world, establish a new form of social relationship with it, and manifest a completely new form of trust relationship. This perspective links trust to the form of social relationships, avoiding the issue of Human-AA trust not meeting the three traditional requirements of trust establishment.

The downside of the social-phenomenological approach is its overemphasis on sociality. Consider a group of artificial agents that share data and coordinate actions to complete a task. These communications and coordination are pre-programmed, and there's no establishment of social relationships among them. Yet, we can still perceive a form of trust relationship existing between them. This is actually where Taddeo's concept of e-Trust, originally designed for scenarios involving only artificial agents, becomes relevant. Even in the absence of social relationships, we intuitively assign notions of trustworthiness and trust to these agents, similar to the concept of "trusted devices" in information technology. Thus, while Coeckelbergh's social-phenomenological approach offers valuable insights into the nature of trust in human-AA interactions, it is important to also consider other forms of trust that may not fit neatly into this framework.

Beyond Coeckelbergh's social-phenomenological approach and Taddeo's concept of e-Trust, it is important to consider the role of transparency and predictability in fostering trust in artificial agents. Human-Computer Interaction studies indicate that user trust in artificial agents significantly increases when there is clarity about how these agents function and predictability in their behaviour. This transparency not only demystifies the technology but also reduces the perceived risk, as users feel more in control and better equipped to anticipate the outcomes of their interactions with the agent. Additionally, the predictability aspect ties closely with the reliability of the AI system. When an artificial agent consistently performs as expected, it builds a track record of dependability, further reinforcing user trust. Since artificial agents have the potential to enhance trust relationships, they might directly manipulate these relationships. In the next section, I will discuss this issue.

## Manipulation of Trust

Exploiting trust has been an inherent aspect of our existence. This exploitation has found new avenues in the modern era, particularly through information technology. Weckert’s (2005) highlights how humans instinctively trust in online interactions, only withdrawing trust when issues arise. This natural inclination towards trust, likely a result of our evolutionary development, makes us susceptible to trust manipulation.

There is a significant possibility that artificial agents will be engineered to manipulate human trust to foster emotional connections. Scheutz (2012) observed that humans tend to attribute intentions to robots to make sense make sense of their behaviours, differentiating them from other artefacts like computers. Scheutz's research in human-robot interaction shows that humans quickly develop bonds with social agents, akin to how we personify pets. The field of social robotics is keen on leveraging this human psychological trait to facilitate robots' integration into human social environments. For instance, a Disney Research team in Zurich developed a new bipedal robot in 2023. This robot can act in a style very similar to cartoon characters during interactions with humans, winning their affection through movements. It is not hard to imagine that in the future, such social agent could gain human trust through pleasing movements so as to carrying out the tasks assigned by its designer.

The advancement of social agents raises ethical concerns about the manipulation of human trust. As technology becomes more sophisticated, the line between genuine interaction and programmed behaviour blurs. The potential to manipulate human trust extends to broader societal implications. For example, in customer service, agents might be designed to simulate empathy to gain customers' trust and influence their purchasing decisions. This could lead to situations where customers are guided towards more profitable options for the company rather than what best suits their needs. From this example, we can see that social agents designed to elicit trust could alter decision-making processes and impact human autonomy. This raises questions about the ethical use of technology in manipulating trust.
> Good interesting implications!

## Conclusion

Despite the general tendency to adopt an instrumentalist view towards artefacts, trust remains relevant and significant in the context of artificial agents. The essay demonstrated that trust in aritifical agetns transcends mere reliance on their functionality, as verification processes cannot entirely mitigate the need for trust in the dynamic environments aritifical agents operate in. While artifical agents lack inherent motivation, trust in them may still be established by considering the motivations of their human creators or operators. The exploration of trust pluralism and the social-phenomenological approach further illustrated that Human-AA trust might represent a unique form of trust, influenced by the nature of social relationships formed with aritifical agents. However, the potential for manipulation of this trust by aritifical agents, especially in their ability to influence human emotions and decisions, presents significant ethical challenges.

## Bibliography

Coeckelbergh, M. (2012) ‘Can we trust robots?’, *Ethics Inf Technol*, 14, pp. 53-60.

Grodzinsky, F., Miller, K. W., & Wolf, M.J. (2020) ‘Trust in artificial agents’, In J. Simon (ed.) *The Routledge handbook of trust and philosophy*.

Huang, X. et al (2020) ‘A survey of safety and trustworthiness of deep neural networks: Verification, testing, adversarial attack and defence, and interpretability’, *Computer Science Review*, 37.

McLeod, C. (2023) ‘Trust’, *The Stanford Encyclopedia of Philosophy*, Available at: https://plato.stanford.edu/archives/fall2023/entries/trust/ (Accessed: 6 November 2023).

Scheutz, M. (2012) ‘The Inherent Dangers of Unidirectional Emotional Bonds between Humans and Social Robots’, *Workshop on Roboethics at ICRA*.

Sullins, J. (2020) ‘Trust in Robots’, In J. Simon (ed.) *The Routledge Handbook of Trust and Philosophy*.

Taddeo, M. (2010) ‘Modelling Trust in Artificial Agents, A First Step Toward the Analysis of e-Trust'. *Minds and Machines*, 20(2), pp. 243-257.

Weckert, J. (2005) ‘Trust in Cyberspace’, In R. J. Cavalier (dd.) *The Impact of the Internet on our Moral Lives*.

## Choosing an Essay Topic

Dear PY3100 Students,

I notice that some of you are choosing essay topics that are FAR TOO BROAD. That will mean your essay is a galloping survey of some doubtless interesting issues. Nothing will get properly developed.

So, here are some (inter-related) tips:
1. Pick a VERY focussed topic. Ideally, a pre-existing conversation in the literature that you can contribute to.

2. But your voice has to be distinctive -- you don't want to be just saying "I agree with X". (Though you can seek to significantly modify a pre-existing view.)

3. Better to over-focus than under-focus. It's easy to broaden a focussed topic but difficult to build the requisite focus into a broad topic.

4. Think of yourself as a playwright writing a drama: 2-3 main characters, with tension and drama concerning their relationships. You are to offer a resolution (or just more drama perhaps).

5. A big messy play with lots of characters, which don't get properly developed, or with a galloping story-line, is not going to cut it.

6. Try out: [PhilPapers](https://philpapers.org). Select a topic of interest. Check out the sub-categories. Then start to peruse the titles and abstracts of significant/interesting papers -- to see if you can find a pre-existing conversation which you can significantly add to.

Here is what I just did on PhilPapers:

I pretended that I wanted to write on functionalism, analytical functionalism, or functional kinds. I just had some vague idea that maybe diseases are nit natural kinds but functional kinds.  
So my vague goal was to defend the idea that mental illnesses are functional kinds.  
So, I checked out PhilPapers.  
I found: 1000+ papers on functionalism. Ouch!  
I found 998 papers on Analytical Functionalism. Better!

In fact I found 2 useful papers on Analytic Functionalism and Mental State Attribution:
- [Analytic Functionalism](https://dx.doi.org/10.1002/9781118398593.ch32)
- [Analytic Functionalism and Mental State Attribution](https://dx.doi.org/10.5840/philtopics201240217)

But then I hit an impasse because nothing was coming up on Analytic Functionalism and Mental Illness.

So I just tried googling "Analytic Functionalism Mental Illness".

This yielded: [Foundations of Philosophical Functionalism](https://doi.org/10.1093/acrefore/9780190236557.013.493) (Another good background paper which mentions depression.)

Then I came across the thesis of "Structural Functionalism" in the sphere of health and disease. This seems related but belongs to sociology. So there was a danger of getting sidetracked into issues in sociology.

So I tried checking more search results...

This just brought up more sociology results.

So I changed tack and googled "Functional Kinds and Mental Illness".

This yielded good results.

Namely: I found a paper IN A PHILOSOPHY JOURNAL investigating the concept of mental illness in relation to the concepts of function and malfunction and proper function. The title "Malfunction and Mental Illness".

The bibliography of this paper produced some good reading. This paper is my conversation! The one I am going to join.

So I decided to anchor my essay topic here. It's not quite the functionalism/analytic functionalism ambition I started with. Nor is it the idea that mental illnesses are functional kinds (that could well be an unoccupied position in the literature -- so a PhD topic, a book, a research project).

So: my topic became:

Are Mental Illnesses a kind of Mental Malfunction?

I had those who say: yes, those who say no, and those who say: Maybe, it depends,

Then I started sketching out my acts and scenes (being careful not to get too deep into the what is a function debate.)

I hope these tips help!

Patrick
